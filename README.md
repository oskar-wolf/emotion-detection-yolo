# Emotion Detection with YOLOv8

A simple, end-to-end pipeline for detecting a personâ€™s emotional state in images using Ultralytics YOLOv8. Covers raw data audit, preprocessing, training, detailed evaluation, and an inference demoâ€”organized into Jupyter notebooks.

---

## ğŸ“ Repository Structure
emotion-detection-yolo/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original train/valid/test splits with images & labels
â”‚ â”œâ”€â”€ interim/ # Face-cropped 640Ã—640 images & labels after audit
â”‚ â””â”€â”€ processed/ # Final, model-ready images & labels
â”‚
â”œâ”€â”€ demo_images/ # Your own 9 example selfies for inference demo
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ emotion_detector/
â”‚ â””â”€â”€ weights/ # best.pt & last.pt checkpoints from training
â”‚
â”œâ”€â”€ runs/
â”‚ â”œâ”€â”€ detect/ # detection-task validation outputs
â”‚ â””â”€â”€ inference_demo/ # saved demo inference images & JSON
â”‚
â”œâ”€â”€ notebooks/ # Phase-by-phase Jupyter notebooks
â”‚ â”œâ”€â”€ 1_data_audit.ipynb
â”‚ â”œâ”€â”€ 2_preprocessing.ipynb
â”‚ â”œâ”€â”€ 3_train_and_val.ipynb
â”‚ â”œâ”€â”€ 4_evaluate.ipynb
â”‚ â””â”€â”€ 5_inference_demo.ipynb
â”‚
â”œâ”€â”€ data.yaml # YOLOv8 dataset config (train/val/test paths + class names)
â”œâ”€â”€ requirements.txt # pip install -r requirements.txt
â”œâ”€â”€ yolov8n.pt # pretrained YOLOv8n backbone
â”œâ”€â”€ LICENSE # MIT
â””â”€â”€ README.md # this file


---

## ğŸš€ Quickstart

1. **Clone & install**  
   ```bash
   git clone https://github.com/yourname/emotion-detection-yolo.git
   cd emotion-detection-yolo
   pip install -r requirements.txt

Prepare your data

Download the â€œ8 Facial Expressions for YOLOâ€ dataset from Kaggle/Roboflow

Place the train/, valid/, and test/ folders under data/raw/

Run the notebooks
Launch Jupyter in the project root:

Then execute, in order:

notebooks/1_data_audit.ipynb

notebooks/2_preprocessing.ipynb

notebooks/3_train_and_val.ipynb

notebooks/4_evaluate.ipynb

notebooks/5_inference_demo.ipynb

ğŸ—‚ï¸ Phase Overviews
1. Data Audit
Detect & crop the primary face in each image using OpenCV Haar cascades

Resize crops to 640Ã—640 and save under data/interim/<split>/images

Copy matching labels to data/interim/<split>/labels

Summarize how many images per split were kept

2. Preprocessing
Load interim crops

Normalize pixels to [0,1] and convert any grayscale â†’ RGB

Save final images under data/processed/<split>/images and copy labels

3. Training & Validation
Train a YOLOv8n model on the processed train/ and valid/ splits

Monitor box loss, classification loss, mAP@0.5 and mAP@0.5:0.95

Save best checkpoint to models/emotion_detector/weights/best.pt

4. Detailed Evaluation
Run model.val() on the held-out test split

Print overall and per-class mAP, confusion matrix, precision/recall/F1

5. Inference Demo
Place your own 9 selfies (one per emotion) in demo_images/

Run inference, display the top-2 predictions per image, and save to runs/inference_demo/

ğŸ“Š Results
AP50: ~72 %

mAP50â€“95: ~52 %

Single-label accuracy (per-image): ~80 %

Per-class F1-score range:

Happy: ~0.94

Neutral: ~0.65

âš™ï¸ Configuration
data.yaml defines your dataset paths and class names.

Adjust training hyperparameters directly in notebooks/3_train_and_val.ipynb or supply a custom hyp_custom.yaml to model.train().

ğŸ“œ License
This project is released under the MIT License.
See LICENSE for details.